{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline \n",
    "from sklearn import datasets\n",
    "from sklearn import feature_selection\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# load the dataset\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. import and instantiate CountVectorizer (with the default parameters)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 2. instantiate CountVectorizer (vectorizer)\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic = rec.motorcycles\n",
      "\n",
      "/ hpcc01:rec.motorcycles / Stafford@Vax2.Winona.MSUS.Edu (John Stafford) / 11:06 am  Apr  1, 1993 /\n",
      " \n",
      "\n",
      "  It depreciates much faster, too.\n",
      "   \n",
      "====================================================\n",
      "John Stafford   Minnesota State University @ Winona\n",
      "                    All standard disclaimers apply.\n",
      "----------\n",
      "The '84 GL1200A hit the traps at 13.34 according to Cycle magazine. Yeah,\n",
      "they depreciate faster than Harleys for the first couple of years then\n",
      "they bottom out. Got my '86 GL1200I w/ 2275 miles on the odometer for\n",
      "just under $5K in May of 1990 and would ask for $4500 now with almost\n",
      "16K miles onnit....that's about 50% of what a new GL1500I would cost.\n",
      "\n",
      "Think the '86 GL1200I originally sold for $6500 brand new, not sure. \n",
      "If that's the case then it depreciated 30.77% over 7 years or a mere\n",
      "$2000. Big Fat Hairy Deal! Based on what I know, Harleys tend to\n",
      "depreciate your monies far more than the initial depreciation of\n",
      "the bike itself when it comes to parts and service. All this about\n",
      "Harleys holding their value better doesn't always wash away the\n",
      "knocks on them...such as being much slower. ;-) \n",
      "\n",
      "According to Peter Egan in the just released Cycle World his FLHS is a\n",
      "real dog when he pillions his 120lb wife. All that money for a dog that\n",
      "doesn't defecate much. =:-]  \n",
      "--------------------------------------------------------------------------\n",
      "Graeme Harrison, Hewlett-Packard Co., Communications Components Division,\n",
      "350 W Trimble Rd, San Jose, CA 95131 (gharriso@hpcc01.corp.hp.com) DoD#649 \n"
     ]
    }
   ],
   "source": [
    "n = 102\n",
    "print('Topic = {0}\\n'.format(newsgroups_train.target_names[newsgroups_train.target[n]]))\n",
    "print(newsgroups_train.data[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=None, analyzer='word', binary=True)\n",
    "vectorizer.fit(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101631"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 101631)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 101631)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(binary=False)\n",
    "X_train_counts = count_vect.fit_transform(newsgroups_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB().fit(X_train_counts, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "X_test = vectorizer.transform(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(binary=False)\n",
    "X_train_counts = count_vect.fit_transform(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB().fit(X_train_counts, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "count_vect = CountVectorizer(stop_words=ENGLISH_STOP_WORDS, binary=False)\n",
    "X_train_counts = count_vect.fit_transform(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X_train_counts, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS).fit(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09418459, 0.13703598, 0.25808578, 0.16368393, 0.16329311,\n",
       "       0.11339407, 0.14613089, 0.12706904, 0.12197187, 0.08978258,\n",
       "       0.1614203 , 0.13037295, 0.10043854, 0.10634736, 0.13520842,\n",
       "       0.13822597, 0.06961998, 0.11869933, 0.12504221, 0.2245489 ,\n",
       "       0.20599311, 0.14341273, 0.12667096, 0.17300821, 0.1484788 ,\n",
       "       0.10526009, 0.46579831, 0.10548299, 0.19644481, 0.24723135,\n",
       "       0.12937103, 0.14077746, 0.20599311, 0.20797701])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectors = vectorizer.transform(newsgroups_train.data)\n",
    "X_train_vectors[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 101322)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 101322)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_transformer = TfidfTransformer()\n",
    "X_train_tf = tf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tf, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Jesus' => soc.religion.christian\n",
      "'The Solar Systemis the gravitationally bound planetary system of the Sun ' => sci.space\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['Jesus', 'The Solar Systemis the gravitationally bound planetary system of the Sun ']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "     print('%r => %s' % (doc, newsgroups_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(newsgroups_train.data, newsgroups_train.target)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6062134891131173"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
    "docs_test = newsgroups_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == newsgroups_test.target)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6836165693043016"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None)),\n",
    " ])\n",
    "text_clf.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == newsgroups_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.56      0.42      0.48       319\n",
      "           comp.graphics       0.69      0.67      0.68       389\n",
      " comp.os.ms-windows.misc       0.67      0.60      0.63       394\n",
      "comp.sys.ibm.pc.hardware       0.65      0.65      0.65       392\n",
      "   comp.sys.mac.hardware       0.76      0.68      0.72       385\n",
      "          comp.windows.x       0.74      0.71      0.73       395\n",
      "            misc.forsale       0.48      0.85      0.61       390\n",
      "               rec.autos       0.79      0.70      0.74       396\n",
      "         rec.motorcycles       0.73      0.77      0.75       398\n",
      "      rec.sport.baseball       0.82      0.78      0.80       397\n",
      "        rec.sport.hockey       0.82      0.91      0.86       399\n",
      "               sci.crypt       0.71      0.74      0.73       396\n",
      "         sci.electronics       0.67      0.49      0.57       393\n",
      "                 sci.med       0.76      0.79      0.78       396\n",
      "               sci.space       0.70      0.76      0.73       394\n",
      "  soc.religion.christian       0.61      0.82      0.70       398\n",
      "      talk.politics.guns       0.56      0.70      0.62       364\n",
      "   talk.politics.mideast       0.74      0.82      0.78       376\n",
      "      talk.politics.misc       0.70      0.35      0.47       310\n",
      "      talk.religion.misc       0.52      0.12      0.20       251\n",
      "\n",
      "               micro avg       0.68      0.68      0.68      7532\n",
      "               macro avg       0.68      0.67      0.66      7532\n",
      "            weighted avg       0.69      0.68      0.67      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(newsgroups_test.target, predicted,\n",
    "     target_names=newsgroups_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[135,   0,   2,   2,   0,   2,  13,   4,   9,   5,   4,   4,   4,\n",
       "          8,  20,  73,   6,  20,   0,   8],\n",
       "       [  7, 262,  20,  12,   7,  22,  12,   1,   5,   3,   0,  14,   5,\n",
       "          2,  13,   1,   1,   2,   0,   0],\n",
       "       [  2,  15, 238,  34,  18,  23,  18,   1,   3,   4,   2,   4,   1,\n",
       "          8,  11,   1,   4,   1,   4,   2],\n",
       "       [  0,  14,  26, 256,  21,   9,  25,   2,   1,   1,   2,   9,  21,\n",
       "          1,   1,   0,   0,   2,   1,   0],\n",
       "       [  0,   6,   6,  35, 261,   7,  30,   6,   9,   0,   3,   5,   9,\n",
       "          2,   4,   1,   1,   0,   0,   0],\n",
       "       [  0,  37,  33,   4,   6, 282,  14,   0,   2,   1,   0,   6,   3,\n",
       "          1,   5,   0,   0,   1,   0,   0],\n",
       "       [  0,   3,   0,  12,   9,   0, 330,   7,   6,   2,   2,   1,   5,\n",
       "          0,   6,   1,   5,   1,   0,   0],\n",
       "       [  6,   1,   4,   1,   1,   3,  37, 276,  21,   3,   3,   2,  16,\n",
       "          2,   5,   0,   8,   5,   2,   0],\n",
       "       [  2,   0,   1,   1,   1,   0,  24,  16, 307,   4,   0,   1,   5,\n",
       "          7,   8,   4,   6,   5,   4,   2],\n",
       "       [  3,   1,   0,   0,   1,   1,  22,   2,   5, 311,  36,   1,   1,\n",
       "          2,   3,   4,   0,   0,   4,   0],\n",
       "       [  2,   2,   0,   1,   0,   0,   9,   0,   2,   7, 364,   1,   0,\n",
       "          2,   2,   1,   4,   0,   0,   2],\n",
       "       [  3,   5,   5,   3,   4,   6,  20,   1,   3,   6,   1, 294,   5,\n",
       "          4,   7,   2,  15,   6,   4,   2],\n",
       "       [  1,  11,   8,  22,   8,  11,  32,  11,  14,   7,   5,  37, 194,\n",
       "         14,  12,   2,   2,   2,   0,   0],\n",
       "       [  1,   7,   1,   0,   0,   2,  23,   3,   5,   1,   7,   2,   7,\n",
       "        312,   3,   7,   5,   5,   4,   1],\n",
       "       [  4,   7,   3,   1,   1,   2,  22,   5,   4,   3,   2,   2,   9,\n",
       "          9, 301,   3,   4,   7,   5,   0],\n",
       "       [ 14,   3,   2,   1,   0,   2,  15,   0,   1,   4,   2,   1,   2,\n",
       "          6,   6, 325,   2,   6,   2,   4],\n",
       "       [  3,   2,   4,   3,   2,   2,  16,   3,   7,   4,   1,  16,   0,\n",
       "          6,   6,  12, 253,   9,   9,   6],\n",
       "       [ 13,   2,   2,   0,   0,   2,   8,   1,   5,   6,   1,   3,   1,\n",
       "          2,   1,   7,  10, 308,   4,   0],\n",
       "       [ 11,   0,   0,   2,   1,   2,   7,   5,   6,   4,   7,   9,   1,\n",
       "          8,  13,   5,  98,  20, 109,   2],\n",
       "       [ 32,   4,   2,   1,   1,   1,  11,   6,   5,   1,   4,   2,   1,\n",
       "         13,   4,  85,  28,  15,   4,  31]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(newsgroups_test.target, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-993d1b4c4dd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m      ('clf', LogisticRegressionCV (loss='hinge', penalty='l2',\n\u001b[0;32m     11\u001b[0m                            \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                            max_iter=5, tol=None)),\n\u001b[0m\u001b[0;32m     13\u001b[0m  ])\n\u001b[0;32m     14\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_counts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewsgroups_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'loss'"
     ]
    }
   ],
   "source": [
    "x= vectorizer.fit_transform(newsgroups_train.data)\n",
    "y= newsgroups_train.target\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV      # using multiNomial Naive Bayes as classifier\n",
    "\n",
    "clf= LogisticRegressionCV()\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "     ('clf', LogisticRegressionCV (loss='hinge', penalty='l2',\n",
    "                           alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None)),\n",
    " ])\n",
    "clf.fit(X_train_counts, newsgroups_train.target)\n",
    "pred= clf.predict(newsgroups_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultinomialNB' object has no attribute 'Cs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-274ec7599f7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'MultinomialNB' object has no attribute 'Cs'"
     ]
    }
   ],
   "source": [
    "clf.Cs   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report , accuracy_score\n",
    "print('accuracy=',accuracy_score( newsgroups_train.target,pred))\n",
    "print(classification_report( newsgroups_train.target, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
